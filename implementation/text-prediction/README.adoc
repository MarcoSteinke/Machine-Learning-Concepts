# Text Prediction using Bigrams and Markov Models:

## 1. Introduction:

For the implementation of text prediction I am using the concept of Markov Models, which allows me to calculate the probabilities of consecutively events.
I will first explain what a Markov Model is.

## 2. Hidden Markov Model:

To explain a Markov Model it is important to start by understanding what a Markov Chain is.

### 2.1 Markov-Chain
A Markov Chain is a stochastic model, which models a sequence of random variables. It is assumed that states in the future 
only depend on a limited amount of previous states (Markov Adoption)

Let q1, q2, ..., qn be a series of states. With the Markov Adoption you can assume:

image::markovchain.PNG[]

So instead of spectating the probabilities of all previous states of the series, you may only spectate the previous state and end up with the same result.

## 3. Text prediction:

After introducing the idea of hidden Markov models and Markov chains I can now proceed and presentate the appliance of this concept.

### 3.1 N-grams:

Let the next sentence be the ongoing example for this section.

"On a rainy day please carry an umbrella."

Following the idea of Markov chains, you are interested in the following probabilities:

- P( a        | On     )
- P( rainy    | a      )
- P( day      | rainy  )
- P( please   | day    )
- P( carry    | please )
- P( an       | carry  )
- P( umbrella | an     )

Therefore you will have to understand the structure of each probability:

P(day, rainy) defines the probability of the word "day" following the word "rainy" in a given input. You can use the following synonyms: 

P(day,rainy) -> next = day, previous = rainy.

To get this probability, you will have to count the amount of the value of `previous` in the given input and then count how often the value of `next` is following the value of `previous` in the given input.

The quotient of them gives you the probability as follows: 

P(next, previous = (|next| / |previous|).

This will get clear using the following quote:

"To be or not to be"

What is the probability P(be, to) ?

Ignoring the casing of the words (you could format all words to lowercase beforehand) you will find "to" being 2 times in the given input and "be" being following "to" 2 times, so you get the probability P(be, to) = 2/2 = 1.

Following this scheme you can now calculate the probabilites for all pairs of words.

If you use this type of probabilities where exactly two words are required to calculate a probability, then you are using `Bigrams` or `2-grams`.

You can generalize this idea by defining `N-grams`, where a N-gram is the probability which you can again find in the Markov chain:

image::markovchain.PNG[]

In the text example, you would find a probability such as:

P(be, To be or not to) = 1

where your `next`-value is a single word and the `previous`-value is a sequence of words with the length `n-1`.

### 3.2 Application of N-grams to text prediction:

Now to predict text you will need the following components:

#### Input:

You will need any sort of textarea or predefined string (for texts it may be a multiline string or a list). This input has to be processed and stored, such that any algorithm can iterate through the result and built bigrams or N-grams from it.

In this implementation I read the input from a textarea and defined an array of separators.

```javascript
static SEPARATORS = ['.', ',', '!', '?', '', "\n"];
```

Those separators will be used to determine when a sentence or paragraph is ending and there shall not be instantiated a bigram from the input at the given location.

##### Example:

"One apple please. I also want a bag."

This would lead to the construction of the bigram (I, please) with P(I | please), which is not really included in the given input.

To avoid such wrong bigrams I am checking each word for not being a separator.

After checking for separators you can iterate through the text and process the input:

```javascript
static formatInput(input) {
        // (1)
        Bigram.input = [];

        Bigram.input = input
            .replaceAll("^", "$")
            .replaceAll(", ", ",")
            .replaceAll(". ", ".")
            .replaceAll("! ", "!")
            .replaceAll("? ", "?")
            .replaceAll(",", "^,^")
            .replaceAll(".", "^.^")
            .replaceAll("!", "^!^")
            .replaceAll("?", "^?^")
            .replaceAll("\n ", "\n")
            .replaceAll("\n", "^\n^")
            .replaceAll(" ", "^")
            .split("^");

        // (2)
        let i = 1;
        while(i < Bigram.input.length) {
            if(Bigram.isSeparator(Bigram.input[Bigram.input.length - 1])) {
                Bigram.input.pop();
            }
            i++;
        }
    } 
```

In the first part of the method (1) I am replacing all separators with spaces around them with the exact same separator, but without spaces. Then I replace the separator with "^^" around it, so I can later split the input and remain the separators in my processed data.

Afterwards I am checking the end of the data for a separator, because I do not want to have any unexpected behaviour of my algorithm when coming to the end, where a separator is the last piece of input and no other word is following it.
