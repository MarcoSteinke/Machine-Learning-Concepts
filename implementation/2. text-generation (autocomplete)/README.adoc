# Text Generation using the Bigrams from last post:

## 1. Introduction:

Since the text prediction was implemented using probabilities for consecutive words, which are being represented in bigrams following the
concepts of the Markov chain, today you will find out how to use the probabilities to generate text from existing text and also how to implement
a basic version of auto-completion, which you already know from your smartphones.

## 2. Concept:

How can you use the probabilities to recommend a word to a user? Let us first look what we achieved yet:

> - P(Ring | One) = 1
- P(to | Ring) = 1
- P(rule | to) = 0.3333333333333333
- P(them | rule) = 1
- P(all | them) = 0.5
- P(find | to) = 0.3333333333333333
- P(them | find) = 1
- P(bring | to) = 0.3333333333333333
- P(them | bring) = 1
- P(in | and) = 1
- P(the | in) = 1
- P(darkness | the) = 1
- P(bind | darkness) = 1
- P(them | bind) = 1'

Those were the probabilities which were calculated from the given input. Now there needs be be another input field, which represents the
user input from any kind of application, e.g. a smartphone keyboard.

Now each time the user finishes a word and hits space the new algorithm has to find the most probable word, which has the current latest word as 
its `previous-value`.

Therefore you will need to iterate through all bigrams and filter for their `previous-value` being equal to the latest word in the user input.

___

### You have now learned how to use the bigrams to generate text and to implement auto-completion.

In the next project you will learn what weaknesses this implementation has and how to improve it by combining bigrams or 2-grams and other n-grams,
to achieve best results in prediction and make the predictions more specific to the current context.

If you want to have a look at the actual implementation, feel free to look at the files included in this directory or to visit:

https://bestofcode.net/Applications/text-generation !

Thank you :)
