# Text Generation using the Bigrams from last post:

## 1. Introduction:

Since the text prediction was implemented using probabilities for consecutive words, which are being represented in bigrams following the
concepts of the Markov chain, today you will find out how to use the probabilities to generate text from existing text and also how to implement
a basic version of auto-completion, which you already know from your smartphones.

## 2. Concept:

How can you use the probabilities to recommend a word to a user? Let us first look what we achieved yet:

> -The final result can be seen in the following probabilities:
- P(Ring | One) = 1
- P(to | Ring) = 1
- P(rule | to) = 0.3333333333333333
- P(them | rule) = 1
- P(all | them) = 0.5
- P(find | to) = 0.3333333333333333
- P(them | find) = 1
- P(bring | to) = 0.3333333333333333
- P(them | bring) = 1
- P(in | and) = 1
- P(the | in) = 1
- P(darkness | the) = 1
- P(bind | darkness) = 1
- P(them | bind) = 1'

___

### You have now learned how to use the bigrams to generate text and to implement auto-completion.

In the next project you will learn what weaknesses this implementation has and how to improve it by combining bigrams or 2-grams and other n-grams,
to achieve best results in prediction and make the predictions more specific to the current context.

If you want to have a look at the actual implementation, feel free to look at the files included in this directory or to visit:

https://bestofcode.net/Applications/text-generation !

Thank you :)
